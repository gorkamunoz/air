{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.abstract_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from fastai.vision.all import DataLoaders\n",
    "from torch.utils.data import TensorDataset, DataLoader as DataLoader_torch   \n",
    "import sklearn\n",
    "import numpy as np\n",
    "from air.losses_metrics import get_mig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21247bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import nbdev_export\n",
    "nbdev_export()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab0be4",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c0b7a-4dcf-4b2e-afaa-1218396ff995",
   "metadata": {},
   "source": [
    "## Normal dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e5e7-f160-4c24-bc74-0fbf8799c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_functions(x, m, seed = 0):\n",
    "    '''\n",
    "    Generate m random nonlinear functions of input x using:\n",
    "    \n",
    "    f_i(x) = sin(w_i^T x + b_i)\n",
    "    \n",
    "    where w_i and b_i are randomly sampled weights and biases.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features)\n",
    "        m: Number of random functions to generate\n",
    "        seed: Random seed for reproducibility\n",
    "    Returns:\n",
    "        Tensor of shape (n_samples, m) with the outputs of the m random functions\n",
    "    '''\n",
    "\n",
    "    torch.random.manual_seed(seed)  # For reproducibility    \n",
    "    k = x.shape[-1]\n",
    "    # Create random weights and biases for m functions, each depending on all k inputs\n",
    "    weights = torch.randn(m, k)\n",
    "    biases = torch.randn(m)\n",
    "    # Apply a nonlinear function (e.g., sin) to a weighted sum of inputs\n",
    "    return torch.sin(x @ weights.t() + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc337472-b105-4c9b-a582-b881f18aef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_abstract(N, num_h = 4, dim_x = None, dim_y = 10, size_train = 0.8, BS = 100,\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                    seed_funcs = 0\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset for the abstract experiment.\n",
    "    \n",
    "    Parameters:\n",
    "    - N: Number of samples\n",
    "    - num_h: Number of hidden factors\n",
    "    - dim_y: Dimensionality of the output\n",
    "    - size_train: Proportion of data to use for training\n",
    "    - BS: Batch size\n",
    "    - device: Device to which tensors will be moved (CPU or GPU)\n",
    "    \n",
    "    Returns:\n",
    "    - DataLoaders for training and testing datasets\n",
    "    \"\"\"\n",
    "\n",
    "    torch.random.manual_seed(0)  # For reproducibility\n",
    "    h = torch.rand((N, num_h))\n",
    "\n",
    "    y1 = random_functions(h[:,:2], dim_y, seed_funcs)\n",
    "    y2 = random_functions(h[:,1:], dim_y, seed_funcs)\n",
    "\n",
    "    if dim_x is None:\n",
    "        x = h.clone()\n",
    "    else:\n",
    "        x = random_functions(h.clone(), dim_x, seed_funcs)     \n",
    "\n",
    "    # Resetting torch seed\n",
    "    torch.seed()\n",
    "\n",
    "\n",
    "    a1 = torch.tensor([0,1]).repeat((N,1))\n",
    "    a2 = torch.tensor([1,0]).repeat((N,1))\n",
    "\n",
    "    data1 = torch.hstack((x, a1))\n",
    "    data2 = torch.hstack((x, a2))\n",
    "    inputs = torch.vstack((data1, data2))\n",
    "    outputs = torch.vstack((y1, y2))\n",
    "\n",
    "    dataset_size = inputs.shape[0]\n",
    "\n",
    "    # Create dataset as inputs (all data) and outputs (only trajectory, no action))\n",
    "    dataset = TensorDataset(inputs[:int(size_train*dataset_size)].to(device),\n",
    "                            outputs[:int(size_train*dataset_size)].to(device))    \n",
    "\n",
    "    # Same for test set\n",
    "    dataset_test = TensorDataset(inputs[int(size_train*dataset_size):].to(device),\n",
    "                                outputs[int(size_train*dataset_size):].to(device))  \n",
    "\n",
    "    # Now define the dataloaders\n",
    "    loader = DataLoader_torch(dataset, batch_size = BS, shuffle = True)\n",
    "    loader_test = DataLoader_torch(dataset_test, batch_size = BS, shuffle = True)\n",
    "\n",
    "    return DataLoaders(loader, loader_test), loader_test, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset_abstract(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243d305-0348-4252-903c-5303a14ed37c",
   "metadata": {},
   "source": [
    "## No actions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ba6c9-2a3d-4905-8e61-1b46588c89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_abstract_no_actions(N, num_h = 4, dim_x = 10, size_train = 0.8, BS = 100,\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                    seed_funcs = 0, torch_seed = None\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset for the abstract experiment. In this case input is same as output as there are no \n",
    "    actions.\n",
    "    \n",
    "    Parameters:\n",
    "    - N: Number of samples\n",
    "    - num_h: Number of hidden factors\n",
    "    - size_train: Proportion of data to use for training\n",
    "    - BS: Batch size\n",
    "    \n",
    "    Returns:\n",
    "    - DataLoaders for training and testing datasets\n",
    "    \"\"\"\n",
    "\n",
    "    if torch_seed is None:\n",
    "        torch.seed()  # For reproducibility\n",
    "    else:\n",
    "        torch.random.manual_seed(torch_seed)  # For reproducibility\n",
    "        \n",
    "    h = torch.rand((N, num_h))\n",
    "\n",
    "    x = random_functions(h.clone(), dim_x, seed_funcs) \n",
    "\n",
    "    # Resetting torch seed\n",
    "    torch.seed()\n",
    "\n",
    "    dataset_size = x.shape[0]\n",
    "    \n",
    "    # Create dataset as inputs (all data) and outputs (only trajectory, no action))\n",
    "    dataset = TensorDataset(x[:int(size_train*dataset_size)].to(device),\n",
    "                            x[:int(size_train*dataset_size)].to(device))    \n",
    "\n",
    "    # Same for test set\n",
    "    dataset_test = TensorDataset(x[int(size_train*dataset_size):].to(device),\n",
    "                                 x[int(size_train*dataset_size):].to(device))  \n",
    "\n",
    "    # Now define the dataloaders\n",
    "    loader = DataLoader_torch(dataset, batch_size = BS, shuffle = True)\n",
    "    loader_test = DataLoader_torch(dataset_test, batch_size = BS, shuffle = True)\n",
    "\n",
    "    return DataLoaders(loader, loader_test), loader_test, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04869230-2f0d-466e-9e4a-651e5c3bbf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = dataset_abstract_no_actions(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfda51b",
   "metadata": {},
   "source": [
    "# MIG abstract dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8936e-7bdc-442f-8216-797479753c10",
   "metadata": {},
   "source": [
    "## Data generation for MIG computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4ec4f-01cb-4d56-809d-d813fbab0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_data_mig(N, num_h, seed_funcs, bins, dim_x):\n",
    "    \n",
    "    '''\n",
    "    Creates the data needed to compute the Mutual Information Gap (MIG) for the abstract experiment \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - N: Number of samples\n",
    "    - num_h: Number of hidden factors\n",
    "    - seed_funcs: Seed for random functions\n",
    "    - bins: Bins for discretization\n",
    "    - dim_x: Dimension of the observation space\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - true_factors: The true hidden factors\n",
    "    - data: The generated dataset with observations and actions\n",
    "    - entropy: Entropy of each hidden factor\n",
    "    '''\n",
    "    \n",
    "    true_factors = torch.rand(N, num_h)\n",
    "\n",
    "    entropy = np.zeros((num_h))\n",
    "    for idx in range(num_h):\n",
    "        cj = true_factors[:, idx].numpy()\n",
    "        cj = np.digitize(cj, np.histogram(cj, bins = bins)[1][:-1])\n",
    "        entropy[idx] = sklearn.metrics.normalized_mutual_info_score(cj, cj)\n",
    "\n",
    "    assert (entropy == 1).all()\n",
    "        \n",
    "    observations = random_functions(true_factors.clone(), dim_x, seed_funcs)\n",
    "    \n",
    "    a1 = torch.tensor([0,1]).repeat((N,1))\n",
    "    a2 = torch.tensor([1,0]).repeat((N,1))\n",
    "    \n",
    "    data1 = torch.hstack((observations, a1))\n",
    "    data2 = torch.hstack((observations, a2))\n",
    "    \n",
    "    data = torch.vstack((data1, data2))\n",
    "\n",
    "    return true_factors, data, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb615f6-6ca5-4c9d-97dc-201a4aaef891",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_factors, data, _ = create_data_mig(N = 10, num_h = 4, seed_funcs = 3, bins = 20, dim_x = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69064873-0425-4c04-a52d-0c29b0b34262",
   "metadata": {},
   "source": [
    "## Class mig calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c28ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class mig_calc_abs_data():\n",
    "\n",
    "    '''\n",
    "    Class to compute the Mutual Information Gap (MIG) for the abstract experiment dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - N: Number of samples\n",
    "    - num_h: Number of hidden factors\n",
    "    - dim_x: Dimension of the observation space\n",
    "    - seed_funcs: Seed for random functions\n",
    "    - torch_seed: Seed for torch random number generator\n",
    "    - device: Device to which tensors will be moved (CPU or GPU)\n",
    "    - action: Action to be used in the dataset (None, False, or specific action)\n",
    "    - bins: Number of bins for discretization\n",
    "    - normalized_MI: Whether to use normalized mutual information\n",
    "    '''\n",
    "\n",
    "    def __init__(self, N, num_h, dim_x, seed_funcs, \n",
    "                 torch_seed = None, device = 'cuda' if torch.cuda.is_available() else 'cpu', \n",
    "                 action = None, bins = 20,\n",
    "                 normalized_MI = True\n",
    "                ):\n",
    "\n",
    "        \n",
    "\n",
    "        self.N = N\n",
    "        self.dim_x = dim_x\n",
    "        self.num_h = num_h\n",
    "        self.action = action\n",
    "        self.seed_funcs = seed_funcs\n",
    "        self.torch_seed = torch_seed \n",
    "        self.bins = bins\n",
    "        self.normalized_MI = normalized_MI\n",
    "        self.device = device\n",
    "\n",
    "        self.reset_dataset()\n",
    "    \n",
    "    def reset_dataset(self):\n",
    "        \n",
    "        if self.torch_seed is None:\n",
    "            torch.seed()\n",
    "        else:\n",
    "            torch.random.manual_seed(self.torch_seed)\n",
    "\n",
    "        self.true_factors = torch.rand(self.N, self.num_h)\n",
    "\n",
    "        self.entropy = np.zeros((self.num_h))\n",
    "        for idx in range(self.num_h):\n",
    "            cj = self.true_factors[:, idx].numpy()\n",
    "            cj = np.digitize(cj, np.histogram(cj, bins = self.bins)[1][:-1])\n",
    "            self.entropy[idx] = sklearn.metrics.normalized_mutual_info_score(cj, cj)\n",
    "        \n",
    "        observations = random_functions(self.true_factors.clone(), self.dim_x, self.seed_funcs)\n",
    "\n",
    "        # Reset torch seed\n",
    "        torch.seed()        \n",
    "\n",
    "        if self.action is None:                \n",
    "            data1 = torch.hstack((observations, torch.tensor([0,1]).repeat((self.N,1))))\n",
    "            data2 = torch.hstack((observations, torch.tensor([1,0]).repeat((self.N,1))))            \n",
    "            self.data = torch.vstack((data1, data2)).to(self.device)\n",
    "        elif self.action is False:\n",
    "            self.data = observations.to(self.device)        \n",
    "        else:\n",
    "            self.data = torch.hstack((observations.repeat(2,1), torch.tensor(self.action).repeat((2*self.N,1)))).to(self.device)\n",
    "        \n",
    "\n",
    "    def compute_mig(self, \n",
    "                    model, # must 3 outputs, where 2nd and 3rd are mu and logvar\n",
    "                    bins = 20,\n",
    "                    normalized_MI = True,\n",
    "                    only_mig = False,\n",
    "                    disentangled_var = 1,\n",
    "                    compute_with_mu = True\n",
    "                   ):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, mu, logvars = model(self.data)\n",
    "        z = reparametrize(mu, logvars).cpu().detach().numpy()\n",
    "\n",
    "        input_mi = mu.detach().cpu().numpy() if compute_with_mu else z\n",
    "\n",
    "        if self.action == False:\n",
    "            mi, mig = get_mig(input_mi, self.true_factors, self.num_h, bins = self.bins, normalized_MI = self.normalized_MI)            \n",
    "        else:\n",
    "            mi, mig = get_mig(input_mi, self.true_factors.repeat((2,1)), self.num_h, bins = self.bins, normalized_MI = self.normalized_MI)\n",
    "\n",
    "        if only_mig:\n",
    "            return mig\n",
    "        \n",
    "        mi = np.sort(mi, 1)[:, ::-1]\n",
    "\n",
    "        # Expected disentangled neuron\n",
    "        dis_mig = (mi[disentangled_var,0] - mi[disentangled_var,1])/self.entropy[1]\n",
    "\n",
    "        # Rest of neurons\n",
    "        ent_mi = np.delete(mi.copy(), disentangled_var, axis = 0)        \n",
    "        ent_mig = ((ent_mi[:,0]-ent_mi[:,1])/self.entropy[np.delete(np.arange(self.num_h), disentangled_var)]).mean()\n",
    "\n",
    "        return mi, mig, dis_mig, ent_mig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mig_calc = mig_calc_abs_data(N = 20, num_h = 3, dim_x = 16, seed_funcs = 123, action=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
