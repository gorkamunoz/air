{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde3d83-77fd-4e5a-a48e-75873b583400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c031395-3225-40e7-a67c-6b32e59dbccc",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44617882-e599-41f9-a877-6a326be9a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from fastai.callback.core import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdc25f6-57e7-45a4-883c-198ea31014d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e5a62-bf3f-43f0-b8b1-93aa21f8d6bb",
   "metadata": {},
   "source": [
    "# $\\beta$-VAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482820a4-7ff4-42ba-9a20-729a927d30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kl_divergence(mu, \n",
    "                  logvar):\n",
    "    ''' \n",
    "    Computes the D_KL between two normal distributions N(mu, exp(logvar)) and N(0,1) (this last is the prior)\n",
    "    '''\n",
    "    batch_size = mu.size(0)\n",
    "    assert batch_size != 0\n",
    "    if mu.data.ndimension() == 4:\n",
    "        mu = mu.view(mu.size(0), mu.size(1))\n",
    "    if logvar.data.ndimension() == 4:\n",
    "        logvar = logvar.view(logvar.size(0), logvar.size(1))\n",
    "\n",
    "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_kld = klds.sum(1).mean(0, True)\n",
    "    dimension_wise_kld = klds.mean(0)\n",
    "    mean_kld = klds.mean(1).mean(0, True)\n",
    "\n",
    "    return total_kld, dimension_wise_kld, mean_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7749e77-224c-4784-82c0-cb82bdb2f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class beta_mse_loss(torch.nn.modules.loss._Loss):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 loss_objective = 'H', # 'H' for original (Higgins et al.2017), 'B' for version (Burgess et al. 2017)\n",
    "                 beta = None, # Regularizing factor in betaVAE\n",
    "                 gamma = None, # Regularizing factor in Burgess betaVAE\n",
    "                 C_max = None, # Maximum value of C\n",
    "                 C_stop_iter = None, # When to reach the C_max\n",
    "                 recon_objective = 'mse', # either 'mse' (Mean Squared Error) or 'bce' (Binary Cross Entropy)\n",
    "                 reduction = None, # reduction applied in the mse\n",
    "                 mse_as_konny = False # if True, adapts the MSE loss to have exactly what they have in https://github.com/1Konny/Beta-VAE/blob/master/solver.py\n",
    "                 ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if reduction is not None:\n",
    "            self.reduction = reduction\n",
    "        \n",
    "        self.loss_objective = loss_objective\n",
    "    \n",
    "        # For original objective\n",
    "        self.beta = beta\n",
    "        # For update objective\n",
    "        self.gamma = gamma\n",
    "        self.C_max = C_max\n",
    "        self.C_stop_iter = C_stop_iter\n",
    "        self.global_iter = 0\n",
    "\n",
    "        self.mse_as_konny = mse_as_konny\n",
    "        \n",
    "        self.recon_objective = recon_objective\n",
    "        \n",
    "    def forward(self,\n",
    "                input: tuple, # prediction of the model given as recon, mu, logvar\n",
    "                target: torch.Tensor, # target for the reconstruction\n",
    "                separate_loss = False # if giving the two parts of the loss separatedly\n",
    "                ) -> torch.Tensor:        \n",
    "        \n",
    "        # Separate the input into different variables\n",
    "        recon, mu, logvar = input\n",
    "        \n",
    "        if recon.shape != target.shape:\n",
    "            # Most typically, we will have a input that has an extra dimension due to the number of channels\n",
    "            recon = recon.squeeze(1)\n",
    "\n",
    "        #### COMPUTE RECONSTRUCTION LOSS ####\n",
    "        if self.recon_objective == 'mse':\n",
    "            if self.mse_as_konny:\n",
    "                recon = torch.sigmoid(recon)                \n",
    "                rec_loss = torch.nn.functional.mse_loss(recon, target, reduction = 'sum').div(target.shape[0])\n",
    "            else:\n",
    "                rec_loss = torch.nn.functional.mse_loss(recon, target, reduction = self.reduction)\n",
    "            \n",
    "        else:\n",
    "            rec_loss = torch.nn.functional.binary_cross_entropy_with_logits(recon, target, reduction = self.reduction, size_average = False)  \n",
    "\n",
    "        #### COMPUTE LATENT LOSS ####\n",
    "        total_kld, dimension_wise_kld, mean_kld = kl_divergence(mu, logvar)                \n",
    "                       \n",
    "        #### MERGE BOTH LOSSES ####\n",
    "        if self.loss_objective == 'H': # as in original betaVAE\n",
    "            if separate_loss:\n",
    "                return rec_loss, self.beta*total_kld\n",
    "            else:\n",
    "                return rec_loss + self.beta*total_kld\n",
    "\n",
    "        elif self.loss_objective == 'B': # as in Burgess betaVAE\n",
    "\n",
    "            # Calculate the C. Note that you have to update the self.global_iter by means the betaLoss_C_scheduler callback\n",
    "            C = torch.clamp(torch.tensor([self.C_max/self.C_stop_iter*self.global_iter]), 0, self.C_max).to(total_kld.device)\n",
    "\n",
    "            if separate_loss:\n",
    "                return rec_loss, self.gamma*(total_kld-C).abs()\n",
    "            else:\n",
    "                return rec_loss + self.gamma*(total_kld-C).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c871da-33bc-4b70-a349-9a5e3b6ff1f5",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a0a8ab-2b04-43e7-8eb3-2d33484f442c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0535])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((12,32))\n",
    "bs = 14; z_dim = 5\n",
    "mu = torch.zeros((bs, z_dim))#+0.5\n",
    "logvar = torch.zeros_like(mu)+0.2\n",
    "input = (x, mu, logvar)\n",
    "loss = beta_mse_loss(loss_objective='H', beta = 1)\n",
    "loss(input, x, separate_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d330c2-b7b9-4a00-ab41-c1626c1c328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0535])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = beta_mse_loss(loss_objective='B', C_max = 20, gamma = 1, C_stop_iter=100)\n",
    "# Should be equal to previous at iter = 0 if gamma = beta\n",
    "loss(input, x, separate_loss = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6d73d",
   "metadata": {},
   "source": [
    "# TC-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0428cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class btcvae_loss(nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Beta-TC-VAE loss with the same interface as `beta_mse_loss`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_data : int\n",
    "        Total number of training samples (for minibatch-weighted estimates).\n",
    "    alpha : float\n",
    "        Weight for mutual information term I[z;x].\n",
    "    beta : float\n",
    "        Weight for total correlation term TC[z].\n",
    "    gamma : float\n",
    "        Weight for dimension-wise KL term sum_i KL[q(z_i) || p(z_i)].\n",
    "    is_mss : bool\n",
    "        If True, use minibatch stratified sampling (MSS). Else, use weighted (MWS).\n",
    "    steps_anneal : int or None\n",
    "        If set, linearly anneal the gamma term from 0→1 over this many global steps.\n",
    "    recon_objective : {'mse','bce'}\n",
    "    reduction : {'none','mean','sum'} or None\n",
    "    mse_as_konny : bool\n",
    "        Match 1Konny’s Beta-VAE MSE variant (sigmoid+sum/bs).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_data: int,\n",
    "                 alpha: float = 1.0,\n",
    "                 beta: float = 6.0,\n",
    "                 gamma: float = 1.0,\n",
    "                 is_mss: bool = True,\n",
    "                 steps_anneal: int | None = None,\n",
    "                 recon_objective: str = 'mse',\n",
    "                 reduction: str | None = None,\n",
    "                 mse_as_konny: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # config matching your previous loss style\n",
    "        if reduction is not None:\n",
    "            self.reduction = reduction\n",
    "\n",
    "        self.recon_objective = recon_objective\n",
    "        self.mse_as_konny = mse_as_konny\n",
    "\n",
    "        # BTC-VAE specifics\n",
    "        self.n_data = int(n_data)\n",
    "        self.alpha = float(alpha)\n",
    "        self.beta = float(beta)\n",
    "        self.gamma = float(gamma)\n",
    "        self.is_mss = bool(is_mss)\n",
    "\n",
    "        # annealing (mirrors your global_iter pattern)\n",
    "        self.steps_anneal = steps_anneal if steps_anneal is None else int(steps_anneal)\n",
    "        self.global_iter = 0  # update this externally if you want annealing\n",
    "\n",
    "    # ------------------------ helpers ------------------------\n",
    "    @staticmethod\n",
    "    def _log_density_gaussian(x, mu, logvar):\n",
    "        # returns log N(x | mu, diag(exp(logvar))) per-sample, per-dim\n",
    "        return -0.5 * (math.log(2 * math.pi) + logvar + (x - mu) ** 2 / logvar.exp())\n",
    "\n",
    "    @staticmethod\n",
    "    def _matrix_log_density_gaussian(z, mu, logvar):\n",
    "        \"\"\"\n",
    "        Compute pairwise log-prob: for each sample z_b, its log prob under every q(z|x_j).\n",
    "        z:      (B, D)\n",
    "        mu:     (B, D)\n",
    "        logvar: (B, D)\n",
    "        returns: (B, B, D) where [b, j, d] = log q(z_b[d] | x_j)\n",
    "        \"\"\"\n",
    "        B, D = z.shape\n",
    "        z_ = z.unsqueeze(1)         # (B, 1, D)\n",
    "        mu_ = mu.unsqueeze(0)       # (1, B, D)\n",
    "        lv_ = logvar.unsqueeze(0)   # (1, B, D)\n",
    "        return -0.5 * (math.log(2 * math.pi) + lv_ + (z_ - mu_) ** 2 / lv_.exp())\n",
    "\n",
    "    @staticmethod\n",
    "    def _logsumexp(x, dim=-1):\n",
    "        m, _ = torch.max(x, dim=dim, keepdim=True)\n",
    "        return m + torch.log(torch.sum(torch.exp(x - m), dim=dim, keepdim=True))\n",
    "\n",
    "    def _estimate_log_qz_terms(self, z, mu, logvar):\n",
    "        \"\"\"\n",
    "        Estimate:\n",
    "          log_qz      ~ log q(z)            (joint)\n",
    "          log_prod_qzi~ sum_i log q(z_i)    (product of marginals)\n",
    "          log_q_zCx   ~ log q(z|x)          (conditional, per-sample)\n",
    "        Using MWS or MSS estimators as in Chen et al. (2018).\n",
    "        \"\"\"\n",
    "        B, D = z.shape\n",
    "        # log q(z|x): own-sample conditional (sum over dims)\n",
    "        log_q_zCx = self._log_density_gaussian(z, mu, logvar).sum(dim=1)  # (B,)\n",
    "\n",
    "        # pairwise per-dim log q(z_i | x_j)\n",
    "        log_qzi_xj = self._matrix_log_density_gaussian(z, mu, logvar)     # (B, B, D)\n",
    "\n",
    "        # weights\n",
    "        if self.is_mss:\n",
    "            # MSS: uniform over the batch (stratified)\n",
    "            # log(1/B) weights for each j\n",
    "            logiw = -math.log(B)\n",
    "            logiw_vec = z.new_full((B,), logiw)\n",
    "        else:\n",
    "            # MWS: importance weights scaled by dataset size\n",
    "            # log(1/N) for each j, but summed over j in minibatch => add log(B)\n",
    "            # Equivalent to log(N) correction via logsumexp trick\n",
    "            # Here we compute log(1/N) + logsumexp over j ⇒ subtract log(N) after logsumexp\n",
    "            logiw_vec = z.new_full((B,), -math.log(self.n_data))\n",
    "\n",
    "        # joint: log q(z) = log ∑_j q(z|x_j) * w_j\n",
    "        # per-sample: for each b, sum over j then over dims\n",
    "        # log ∑_j exp(∑_d log q(z_b[d] | x_j)) + log w_j\n",
    "        log_qz_j = log_qzi_xj.sum(dim=2)  # (B, B) dim-sum first\n",
    "        log_qz_weighted = log_qz_j + logiw_vec.unsqueeze(0)  # broadcast over j\n",
    "        log_qz = self._logsumexp(log_qz_weighted, dim=1).squeeze(-1)  # (B,)\n",
    "\n",
    "        # product of marginals: ∑_i log q(z_i) with q(z_i) = ∑_j q(z_i|x_j) * w_j\n",
    "        # compute per-dim LSE, then sum over dims\n",
    "        log_qzi_j = log_qzi_xj  # (B, B, D)\n",
    "        log_qzi_weighted = log_qzi_j + logiw_vec.view(1, B, 1)\n",
    "        log_qzi = self._logsumexp(log_qzi_weighted, dim=1).squeeze(1)  # (B, D)\n",
    "        log_prod_qzi = log_qzi.sum(dim=1)  # (B,)\n",
    "\n",
    "        return log_qz, log_prod_qzi, log_q_zCx\n",
    "\n",
    "    def _reconstruction(self, recon, target):\n",
    "        # mirror your handling of shapes/objectives\n",
    "        if recon.shape != target.shape:\n",
    "            recon = recon.squeeze(1)\n",
    "\n",
    "        if self.recon_objective == 'mse':\n",
    "            if self.mse_as_konny:\n",
    "                recon_act = torch.sigmoid(recon)\n",
    "                return F.mse_loss(recon_act, target, reduction='sum').div(target.shape[0])\n",
    "            else:\n",
    "                # use the reduction attribute if user set it, else default 'mean'\n",
    "                reduction = getattr(self, 'reduction', 'mean')\n",
    "                return F.mse_loss(recon, target, reduction=reduction)\n",
    "\n",
    "        # BCE with logits (matches your previous style)\n",
    "        reduction = getattr(self, 'reduction', 'mean')\n",
    "        return F.binary_cross_entropy_with_logits(recon, target, reduction=reduction)\n",
    "\n",
    "    def _anneal(self):\n",
    "        if self.steps_anneal is None or self.steps_anneal <= 0:\n",
    "            return 1.0\n",
    "        # linear 0→1 over steps_anneal using self.global_iter\n",
    "        t = min(max(self.global_iter, 0), self.steps_anneal)\n",
    "        return float(t) / float(self.steps_anneal)\n",
    "\n",
    "    # ------------------------ public API ------------------------\n",
    "    def forward(self,\n",
    "                input: tuple,           # (recon, mu, logvar)\n",
    "                target: torch.Tensor,   # reconstruction target\n",
    "                separate_loss: bool = False\n",
    "                ) -> torch.Tensor:\n",
    "\n",
    "        recon, mu, logvar = input\n",
    "\n",
    "        # 1) reconstruction loss\n",
    "        rec_loss = self._reconstruction(recon, target)\n",
    "\n",
    "        # 2) sample z ~ q(z|x) using reparam trick\n",
    "        eps = torch.randn_like(mu)\n",
    "        z = mu + eps * (0.5 * logvar).exp()\n",
    "\n",
    "        # 3) compute MI, TC, and dimension-wise KL pieces\n",
    "        log_pz = (-0.5 * (math.log(2 * math.pi) + z ** 2)).sum(dim=1)  # standard normal prior\n",
    "        log_qz, log_prod_qzi, log_q_zCx = self._estimate_log_qz_terms(z, mu, logvar)\n",
    "\n",
    "        mi = (log_q_zCx - log_qz).mean()           # I[z;x]\n",
    "        tc = (log_qz - log_prod_qzi).mean()        # TC[z]\n",
    "        dwkl = (log_prod_qzi - log_pz).mean()      # ∑ KL[q(z_i)||p(z_i)]\n",
    "\n",
    "        # 4) annealed gamma on dwkl (optional)\n",
    "        anneal = self._anneal()\n",
    "\n",
    "        reg = self.alpha * mi + self.beta * tc + anneal * self.gamma * kl_divergence(mu, logvar)[0]\n",
    "        total = rec_loss + reg\n",
    "\n",
    "        if separate_loss:\n",
    "            return rec_loss, reg\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ed041-bac2-4f92-9e48-1c87e8b07b25",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058ab55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((50,32))\n",
    "bs = x.shape[0]; z_dim = 5\n",
    "mu = torch.zeros((bs, z_dim))#+0.5\n",
    "logvar = torch.zeros_like(mu)+0.2\n",
    "input = (x+torch.randn_like(x)*0.5, mu, logvar)\n",
    "loss = beta_mse_loss(loss_objective='H', beta = 1)\n",
    "\n",
    "\n",
    "loss_tc = btcvae_loss(n_data = x.shape[0], \n",
    "                        alpha = 0,\n",
    "                        beta = 0,\n",
    "                        gamma = 1)\n",
    "\n",
    "\n",
    "assert loss(input, x, separate_loss = True) == loss_tc(input, x, separate_loss = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcfa67",
   "metadata": {},
   "source": [
    "# Mutual Information Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_mig(z, inp, true_dimensions, bins = 20, return_entropy = False, normalized_MI = False):\n",
    "\n",
    "    z_dim = z.shape[-1]\n",
    "    mi = np.zeros((true_dimensions, z_dim))\n",
    "    entropy = []\n",
    "    for idx_z in range(z_dim):\n",
    "        zi = z[:,idx_z]\n",
    "        zi_d = np.digitize(zi, np.histogram(zi, bins = bins)[1][:-1])\n",
    "    \n",
    "        for idx_c in range(true_dimensions):\n",
    "            cj = inp[:,idx_c].cpu().detach().numpy()\n",
    "            cj = np.digitize(cj, np.histogram(cj, bins = bins)[1][:-1])\n",
    "            if normalized_MI:\n",
    "                mi[idx_c, idx_z] = sklearn.metrics.normalized_mutual_info_score(cj, zi_d)  \n",
    "            else:        \n",
    "                mi[idx_c, idx_z] = sklearn.metrics.mutual_info_score(cj, zi_d)   \n",
    "            \n",
    "            # Calculate the entropy (just once)\n",
    "            if idx_z == 0:\n",
    "                if normalized_MI:\n",
    "                    entropy.append(sklearn.metrics.normalized_mutual_info_score(cj, cj))\n",
    "                else:\n",
    "                    entropy.append(sklearn.metrics.mutual_info_score(cj, cj))\n",
    "\n",
    "    mi_s = mi.copy()\n",
    "    mi_s.sort(axis = -1)\n",
    "    mig = np.mean((mi_s[:,-1]-mi_s[:,-2])/np.array(entropy))\n",
    "\n",
    "    if return_entropy:\n",
    "        return mi, mig, entropy\n",
    "    else:        \n",
    "        return mi, mig "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992f7cd-2471-402b-9c6f-78e7fda2be1f",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794499c-aa91-4558-b121-f0f4749d0d4c",
   "metadata": {},
   "source": [
    "## $\\beta$ scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1cd38-d474-4ae6-b96d-4f7f942b3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class betaLoss_C_scheduler(Callback):\n",
    "    '''\n",
    "    Updates the C parameter of the Burgess et al. (2018) version of the betaVAE\n",
    "    '''\n",
    "    def after_batch(self):\n",
    "        self.learn.loss_func.global_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3810af-f4b2-4dbb-a311-c42b9c65a8ea",
   "metadata": {},
   "source": [
    "## Saving logvars during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5441954-7423-4274-a458-87ca983f41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class save_logvars(Callback):\n",
    "    \n",
    "    def __init__(self, actions = torch.tensor([[0,1],\n",
    "                                               [1,0]], dtype=torch.float32)\n",
    "                ):\n",
    "        \n",
    "        self.actions = actions\n",
    "        self.num_actions = actions.shape[0]\n",
    "     \n",
    "    def before_fit(self):        \n",
    "        self.logvars = torch.ones((self.learn.n_epoch, self.num_actions, self.learn.model.dim_z))        \n",
    "        \n",
    "    \n",
    "    def after_epoch(self):\n",
    "        \n",
    "        self.logvars[self.learn.epoch] = self.learn.model.E_a(self.actions.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf8c5d-ceb2-4f02-8702-e683c7b88c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class save_air_logvars(Callback):\n",
    "    ''' \n",
    "    Save the logvars for a AIR model with logvars as biases (AIR_v0 type).\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 save_each = 500 # every how many iterations to save the logvars\n",
    "                ):\n",
    "        self.save_each = save_each  \n",
    "        self.save_idx = 0\n",
    "\n",
    "    def before_fit(self):\n",
    "        \n",
    "        self.logvars = torch.zeros(int(len(self.learn.dls[0])*self.learn.n_epoch/self.save_each), \n",
    "                                   self.learn.model.z_dim)\n",
    "        \n",
    "    def after_batch(self):        \n",
    "        if self.learn.iter % self.save_each == 0:\n",
    "            try:\n",
    "                self.logvars[self.save_idx] = self.learn.model.logvar.detach().cpu()\n",
    "                self.save_idx += 1\n",
    "            except:\n",
    "                self.logvars = torch.concat((self.logvars, self.learn.model.logvar.detach().cpu().unsqueeze(0)), dim = 0)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54019e-1719-4a4e-a7d4-1346b1aad7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export         \n",
    "class save_bvae_logvars(Callback):\n",
    "    \n",
    "    '''\n",
    "    Save the logvars for a typical bvae model.\n",
    "    This implies performing a forward pass of the model with a random subset of the dataset.\n",
    "    ''' \n",
    "    \n",
    "    def __init__(self,\n",
    "                 size_batch_logvars = 1000, # size of the training set that will be used for predicting the logvars\n",
    "                 save_each = 500 # every how many iterations to save the logvars\n",
    "                ):\n",
    "        self.size_batch_logvars = size_batch_logvars\n",
    "        self.save_each = save_each\n",
    "        self.save_idx = 0\n",
    "\n",
    "    def before_fit(self):\n",
    "        \n",
    "        self.logvars = torch.zeros(int(len(self.learn.dls[0])*self.learn.n_epoch/self.save_each), \n",
    "                                   self.size_batch_logvars, \n",
    "                                   self.learn.model.z_dim)\n",
    "        \n",
    "    def after_batch(self):        \n",
    "        \n",
    "        if self.learn.iter % self.save_each == 0:\n",
    "            \n",
    "            batch = self.learn.dls.dataset.__getitem__(torch.randint(0, self.dls.dataset.__len__(), size = (self.size_batch_logvars,)))[0]\n",
    "            self.model.eval()\n",
    "            _, _, logvars = self.learn.model.forward(batch)\n",
    "\n",
    "            try:\n",
    "                self.logvars[self.save_idx] = logvars.detach().cpu()            \n",
    "                self.save_idx += 1\n",
    "            except:\n",
    "                self.logvars = torch.concat((self.logvars, logvars.detach().cpu().unsqueeze(0)), dim = 0)\n",
    "            \n",
    "            self.model.train()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749f8b4-b88b-4da4-82a5-4ae0fd55ffb3",
   "metadata": {},
   "source": [
    "## Save separate losses during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class save_separate_losses(Callback):\n",
    "    \n",
    "    def __init__(self, loader_test):\n",
    "        \n",
    "        self.loader_test = loader_test\n",
    "        self.mse_loss = []\n",
    "        self.kl_loss = []\n",
    "\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        \n",
    "        mean_mse, mean_kl = [], []\n",
    "        for batch in self.loader_test:            \n",
    "            pred = self.learn.model(batch[0])\n",
    "            current_loss = self.learn.loss_func.forward(input = pred, target = batch[1], separate_loss= True)\n",
    "            mean_mse.append(current_loss[0].cpu().detach())\n",
    "            mean_kl.append(current_loss[1].cpu().detach())\n",
    "            \n",
    "        self.mse_loss.append(float(torch.mean(torch.tensor(mean_mse))))\n",
    "        self.kl_loss.append(float(torch.mean(torch.tensor(mean_kl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb5f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "action_rep",
   "language": "python",
   "name": "action_rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
