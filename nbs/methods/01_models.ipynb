{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da4e97-2bbe-45e6-8b0b-597da15f6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcfcbd-a6b2-4e0e-8cad-97fa5dd17bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53e5f2-bc86-4f71-8e12-5acbcea3906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba8e65-2d01-4787-a866-3908aaaecef7",
   "metadata": {},
   "source": [
    "# Useful operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be964-3bcb-4b4d-ba6e-a6c5d6463c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn.init as init\n",
    "\n",
    "def kaiming_init(m):\n",
    "    '''\n",
    "    Kaiming initialization of module m\n",
    "    References:\n",
    "    - He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification\n",
    "    '''\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bd0e3-ea3a-4fc4-a914-ed8f36ea2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def reparametrize(mu, logvar):\n",
    "    '''\n",
    "    Reparameterization trick to sample from N(mu, var) from N(0,1).\n",
    "    '''\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78294b2-1d2e-4eb3-95dd-b02ac9b4b2ca",
   "metadata": {},
   "source": [
    "# VAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db111f5-8451-4f43-b8e8-579d6080d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAIR(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Variational AIR (VAIR) architecture. \n",
    "    Consists of:\n",
    "    - Encoder E_x: from observation x to latent mu\n",
    "    - Encoder E_a: from action a to latent logvar\n",
    "    - Decoder: from latent z and action a to reconstruction y\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim_x = 62, # Dimension input to E_z (observation dimension)\n",
    "                 dim_a = 0, # Dimension input to E_a (action dimension)\n",
    "                 dim_enc_h = [512,256], # Dimension layers E_x\n",
    "                 dim_dec_h = [256, 512], # Dimension layers decoder\n",
    "                 dim_z = 2, # Dimension latent                 \n",
    "                 dim_y = 62, # Dimension output \n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define latent dimension\n",
    "        self.dim_z = dim_z \n",
    "        self.dim_a = dim_a\n",
    "        \n",
    "        # E_x\n",
    "        self.lin1 = torch.nn.Linear(dim_x, dim_enc_h[0])        \n",
    "        self.lin2 = torch.nn.Linear(dim_enc_h[0], dim_enc_h[1])   \n",
    "        self.lin3 = torch.nn.Linear(dim_enc_h[1], dim_z)  \n",
    "\n",
    "        # E_a\n",
    "        self.linEa1 = torch.nn.Linear(self.dim_a, 256)            \n",
    "        self.linEa2 = torch.nn.Linear(256, 512) \n",
    "        self.linEa3 = torch.nn.Linear(512, dim_z)         \n",
    "        \n",
    "        # Decoder       \n",
    "        self.lin1_d = torch.nn.Linear(dim_z+dim_a, dim_dec_h[0])        \n",
    "        self.lin2_d = torch.nn.Linear(dim_dec_h[0], dim_dec_h[1])   \n",
    "        self.lin3_d = torch.nn.Linear(dim_dec_h[1], dim_y)              \n",
    "\n",
    "    def E_x(self,\n",
    "            x = None, # dimension: (BS , dim_x)\n",
    "           ):         \n",
    "        mu_p = self.lin1(x)        \n",
    "        mu_p = F.relu(mu_p)\n",
    "        mu_p = self.lin2(mu_p)   \n",
    "        mu_p = F.relu(mu_p)\n",
    "        mu = self.lin3(mu_p)\n",
    "        \n",
    "        return mu \n",
    "\n",
    "\n",
    "    def E_a(self,\n",
    "            a = None, # dimension: (BS , dim_a)\n",
    "           ):        \n",
    "        logvar_p = self.linEa1(a)\n",
    "        logvar_p = F.relu(logvar_p)\n",
    "        logvar_p = self.linEa2(logvar_p)\n",
    "        logvar_p = F.relu(logvar_p)        \n",
    "        logvar = (self.linEa3(logvar_p))\n",
    "        \n",
    "        return logvar  \n",
    "    \n",
    "\n",
    "    def decoder(self,\n",
    "                za = None, # dimension: (BS , dim_z+dim_a)\n",
    "                ):           \n",
    "        \n",
    "        y_p = self.lin1_d(za)\n",
    "        y_p = F.relu(y_p)\n",
    "        y_p = self.lin2_d(y_p) \n",
    "        y_p = F.relu(y_p)        \n",
    "        y = self.lin3_d(y_p)\n",
    "        \n",
    "        return y  \n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Placeholder to mimic VAE encoder in certain analysis\n",
    "        return self.E_x(x)\n",
    "\n",
    "\n",
    "    def forward(self, inp):     \n",
    "\n",
    "        # Separating observation and action from the input\n",
    "        x = inp[:, :-self.dim_a].clone()\n",
    "        a = inp[:, -self.dim_a:].clone()  \n",
    "        \n",
    "        # Encoders pass and sampling latent latent space\n",
    "        mu = self.E_x(x)\n",
    "        logvar = self.E_a(a)\n",
    "        z = reparametrize(mu, logvar)\n",
    "\n",
    "        # Merge action and latent and input to decoder\n",
    "        merge = torch.cat((z, a), axis = 1)         \n",
    "        y = self.decoder(merge)\n",
    "        \n",
    "        return y, mu, logvar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f6d28-c077-41a6-b9c7-e8c130bb43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 5; dim_a = 3; dim_z = 10; dim_y = 3\n",
    "dataset = torch.rand((2, dim_x+dim_a))\n",
    "\n",
    "vair = VAIR(dim_x = dim_x, dim_a = dim_a, dim_y = dim_y)\n",
    "\n",
    "assert vair(dataset)[0].shape == (dataset.shape[0], dim_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a2085-69a3-4b19-b333-76285395695e",
   "metadata": {},
   "source": [
    "# VAE$_{x,a}$: VAE with $x,a$ input, no decoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a78f34-0ec1-449f-a17e-b6e60aaaac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE_xa(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    VAE with observation and action as input to single encoder. No action input to decoder.\n",
    "    Consists of:\n",
    "    - Encoder E_x: from observation x and action a to latent mu and logvar\n",
    "    - Decoder: from latent z to reconstruction y\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim_x = 62, # Dimension input to E_z (observation dimension)\n",
    "                 dim_a = 0, # Dimension input to E_a (action dimension)\n",
    "                 dim_enc_h = [1000,256], # Dimension layers E_x\n",
    "                 dim_dec_h = [256, 512], # Dimension layers decoder\n",
    "                 dim_z = 2, # Dimension latent                 \n",
    "                 dim_y = 62, # Dimension output \n",
    "                ):\n",
    "        super().__init__()        \n",
    "        \n",
    "        # Define latent dimension\n",
    "        self.dim_z = dim_z \n",
    "        self.dim_a = dim_a\n",
    "        \n",
    "        # E_x\n",
    "        self.lin1 = torch.nn.Linear(dim_x+dim_a, dim_enc_h[0])        \n",
    "        self.lin2 = torch.nn.Linear(dim_enc_h[0], dim_enc_h[1])  \n",
    "        self.lin3 = torch.nn.Linear(dim_enc_h[1], 2*dim_z)   \n",
    "        \n",
    "        \n",
    "        # Decoder       \n",
    "        self.lin1_d = torch.nn.Linear(dim_z, dim_dec_h[0])        \n",
    "        self.lin2_d = torch.nn.Linear(dim_dec_h[0], dim_dec_h[1])   \n",
    "        self.lin3_d = torch.nn.Linear(dim_dec_h[1], dim_y)         \n",
    "            \n",
    "\n",
    "    def encoder(self,\n",
    "                xa = None, # dimension: (BS , dim_x+dim_a)\n",
    "                ): \n",
    "        state_part = self.lin1(xa)        \n",
    "        state_part = F.relu(state_part)\n",
    "        state_part = self.lin2(state_part)   \n",
    "        state_part = F.relu(state_part)\n",
    "        state_part = self.lin3(state_part)\n",
    "        return state_part   \n",
    "    \n",
    "\n",
    "    def decoder(self,\n",
    "                z = None, # dimension: (BS , dim_z)\n",
    "                ):           \n",
    "        \n",
    "        y_p = self.lin1_d(z)\n",
    "        y_p = F.relu(y_p)\n",
    "        y_p = self.lin2_d(y_p) \n",
    "        y_p = F.relu(y_p)        \n",
    "        y = self.lin3_d(y_p)\n",
    "        \n",
    "        return y   \n",
    "\n",
    "    def forward(self, inp):            \n",
    "        \n",
    "        # Encoders pass and sampling latent latent space\n",
    "        distributions = self.encoder(xa = inp)\n",
    "        mu = distributions[:, :self.dim_z]\n",
    "        logvar = distributions[:, self.dim_z:]\n",
    "        z = reparametrize(mu, logvar)  \n",
    "\n",
    "        # Decoder pass\n",
    "        y = self.decoder(z)\n",
    "        \n",
    "        return y, mu, logvar\n",
    "\n",
    "\n",
    " # Separating observation and action from the input\n",
    "        x = inp[:, :-self.dim_a].clone()\n",
    "        a = inp[:, -self.dim_a:].clone()  \n",
    "        \n",
    "        # Encoders pass and sampling latent latent space\n",
    "        mu = self.E_x(x)\n",
    "        logvar = self.E_a(a)\n",
    "        z = reparametrize(mu, logvar)\n",
    "\n",
    "        # Merge action and latent and input to decoder\n",
    "        merge = torch.cat((z, a), axis = 1)         \n",
    "        y = self.decoder(merge)\n",
    "        \n",
    "        return y, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117ddb2-a90f-4203-adbc-8afbf3168102",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 5; dim_a = 3; dim_z = 10; dim_y = 3\n",
    "dataset = torch.rand((2, dim_x+dim_a))\n",
    "\n",
    "vae_xa = VAE_xa(dim_x = dim_x, dim_a = dim_a, dim_y = dim_y)\n",
    "\n",
    "assert vae_xa(dataset)[0].shape == (dataset.shape[0], dim_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04361f-ff6e-4592-b97e-5da1552f4271",
   "metadata": {},
   "source": [
    "# VAE$_{D_a}$: VAE with $a$ to decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e3e59-6ce2-493b-8854-cdd0e69210af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE_Da(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    VAE with observation and action as input to single encoder, plus the action is also input to decoder.\n",
    "    Consists of:\n",
    "    - Encoder E_x: from observation x and action a to latent mu and logvar\n",
    "    - Decoder: from latent z and action a to reconstruction y\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim_x = 62, # Dimension input to encoder: the full trajectory (x,y)==((x),(y)); 51x2=102\n",
    "                 dim_enc_h = [1000,256], # Dimension hidden FC\n",
    "                 dim_dec_h = [256, 512], # Dimension hidden FC decoder\n",
    "                 dim_z = 2, # Dimension latent\n",
    "                 dim_a = 0, # Dimension action representation\n",
    "                 dim_y = 62, # Dimension output which is the exact trajectory (x,y)==((x),(y))\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define latent dimension\n",
    "        self.dim_z = dim_z \n",
    "        self.dim_a=dim_a\n",
    "        \n",
    "        # E_x\n",
    "        self.lin1 = torch.nn.Linear(dim_x+dim_a, dim_enc_h[0])        \n",
    "        self.lin2 = torch.nn.Linear(dim_enc_h[0], dim_enc_h[1])   \n",
    "        self.lin3 = torch.nn.Linear(dim_enc_h[1], 2*dim_z)           \n",
    "        \n",
    "        # Decoder       \n",
    "        self.lin1_d = torch.nn.Linear(dim_z+dim_a, dim_dec_h[0])        \n",
    "        self.lin2_d = torch.nn.Linear(dim_dec_h[0], dim_dec_h[1])  \n",
    "        self.lin3_d = torch.nn.Linear(dim_dec_h[1], dim_y)         \n",
    "            \n",
    "\n",
    "    def encoder(self,\n",
    "                xa = None, # dimension: (BS , num_actions , input_size)\n",
    "                ): \n",
    "        \n",
    "        z_p = self.lin1(xa)        \n",
    "        z_p = F.relu(z_p)\n",
    "        z_p = self.lin2(z_p)   \n",
    "        z_p = F.relu(z_p)\n",
    "        z = self.lin3(z_p)\n",
    "        \n",
    "        return z   \n",
    "    \n",
    "\n",
    "    def decoder(self,\n",
    "                za = None, # dimension: (BS , num_actions , input_size)\n",
    "                ):           \n",
    "        \n",
    "        y_p = self.lin1_d(za)\n",
    "        y_p = F.relu(y_p)\n",
    "        y_p = self.lin2_d(y_p) \n",
    "        y_p = F.relu(y_p)\n",
    "        y = self.lin3_d(y_p) \n",
    "        \n",
    "        return y   \n",
    "\n",
    "    def forward(self, inp):  \n",
    "\n",
    "        # Separate action for the decoder\n",
    "        a = inp[:, -self.dim_a:].clone()          \n",
    "\n",
    "        # Encoder pass and latent computations\n",
    "        distributions = self.encoder(inp)\n",
    "        mu = distributions[:, :self.dim_z]\n",
    "        logvar = distributions[:, self.dim_z:]       \n",
    "        z = reparametrize(mu, logvar)  \n",
    "\n",
    "        # Merge latent and actions and decoder pass\n",
    "        merge = torch.cat((z, a), axis = 1)         \n",
    "        y = self.decoder(merge)\n",
    "        \n",
    "        return y, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e85715-7a1b-4b96-b48f-598bc836b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 5; dim_a = 3; dim_z = 10; dim_y = 3\n",
    "dataset = torch.rand((2, dim_x+dim_a))\n",
    "\n",
    "vae_da = VAE_Da(dim_x = dim_x, dim_a = dim_a, dim_y = dim_y)\n",
    "\n",
    "assert vae_da(dataset)[0].shape == (dataset.shape[0], dim_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ec8ca-0c4c-4ba8-9d77-93fa1198b39e",
   "metadata": {},
   "source": [
    "# VAE with any action input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16972d3e-d68f-4b3d-bd8b-c75ad0b6a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Vanilla VAE architecture.\n",
    "    Consists of:\n",
    "    - Encoder E_x: from observation x to latent mu and logvar\n",
    "    - Decoder: from latent z to reconstruction x\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim_x = 62, # Dimension input to encoder\n",
    "                 dim_enc_h = [1000,256], # Dimension hidden FC\n",
    "                 dim_dec_h = [260, 512], # Dimension hidden FC decoder\n",
    "                 dim_z = 2, # Dimension latent\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define latent dimension\n",
    "        self.dim_z = dim_z \n",
    "        \n",
    "        # E_x\n",
    "        self.lin1 = torch.nn.Linear(dim_x, dim_enc_h[0])        \n",
    "        self.lin2 = torch.nn.Linear(dim_enc_h[0], dim_enc_h[1])  \n",
    "        self.lin3 = torch.nn.Linear(dim_enc_h[1], 2*dim_z)   \n",
    "        \n",
    "        \n",
    "        # Decoder       \n",
    "        self.lin1_d = torch.nn.Linear(dim_z, dim_dec_h[0])        \n",
    "        self.lin2_d = torch.nn.Linear(dim_dec_h[0], dim_dec_h[1])   \n",
    "        self.lin3_d = torch.nn.Linear(dim_dec_h[1], dim_x)         \n",
    "            \n",
    "\n",
    "    def encoder(self,\n",
    "                x = None, # dimension: (BS , num_actions , input_size)\n",
    "                ): \n",
    "        \n",
    "        z_p = self.lin1(x)        \n",
    "        z_p = F.relu(z_p)\n",
    "        z_p = self.lin2(z_p)   \n",
    "        z_p = F.relu(z_p)\n",
    "        z = self.lin3(z_p)        \n",
    "        return z\n",
    "    \n",
    "\n",
    "    def decoder(self,\n",
    "                z = None, # dimension: (BS , num_actions , dim_z)\n",
    "                ):           \n",
    "        \n",
    "        y_p = self.lin1_d(z)\n",
    "        y_p = F.relu(y_p)\n",
    "        y_p = self.lin2_d(y_p) \n",
    "        y_p = F.relu(y_p)        \n",
    "        y = self.lin3_d(y_p)        \n",
    "        return y   \n",
    "\n",
    "    def forward(self, inp): \n",
    "\n",
    "        # Encoder pass and latent operations\n",
    "        distributions = self.encoder(x = inp)\n",
    "        mu = distributions[:, :self.dim_z]\n",
    "        logvar = distributions[:, self.dim_z:]       \n",
    "        z = reparametrize(mu, logvar)  \n",
    "\n",
    "        # Decoder pass\n",
    "        y = self.decoder(z)\n",
    "        \n",
    "        return y, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264c59f-fc8c-42a8-8aea-f3073358c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 5; dim_a = 3; dim_z = 10;\n",
    "dataset = torch.rand((2, dim_x))\n",
    "\n",
    "vae = VAE(dim_x = dim_x, dim_z = dim_z)\n",
    "\n",
    "assert vae(dataset)[0].shape == (dataset.shape[0], dim_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
